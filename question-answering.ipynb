{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2025-02-09T07:32:14.498116Z","iopub.execute_input":"2025-02-09T07:32:14.498328Z","iopub.status.idle":"2025-02-09T07:32:15.629639Z","shell.execute_reply.started":"2025-02-09T07:32:14.498307Z","shell.execute_reply":"2025-02-09T07:32:15.628723Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Install necessary libraries\n!pip install evaluate datasets gradio transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T08:01:51.066858Z","iopub.execute_input":"2025-02-09T08:01:51.067073Z","iopub.status.idle":"2025-02-09T08:02:02.514069Z","shell.execute_reply.started":"2025-02-09T08:01:51.067052Z","shell.execute_reply":"2025-02-09T08:02:02.513244Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nCollecting gradio\n  Downloading gradio-5.15.0-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.28.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\nCollecting fastapi<1.0,>=0.115.2 (from gradio)\n  Downloading fastapi-0.115.8-py3-none-any.whl.metadata (27 kB)\nCollecting ffmpy (from gradio)\n  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\nCollecting gradio-client==1.7.0 (from gradio)\n  Downloading gradio_client-1.7.0-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\nRequirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\nCollecting markupsafe~=2.0 (from gradio)\n  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\nRequirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.12)\nRequirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\nRequirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.11.0a1)\nRequirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\nCollecting python-multipart>=0.0.18 (from gradio)\n  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\nCollecting ruff>=0.9.3 (from gradio)\n  Downloading ruff-0.9.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\nCollecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\nCollecting semantic-version~=2.0 (from gradio)\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\nCollecting starlette<1.0,>=0.40.0 (from gradio)\n  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\nCollecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.1)\nRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\nCollecting uvicorn>=0.14.0 (from gradio)\n  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.7.0->gradio) (14.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\nRequirement already satisfied: pydantic-core==2.28.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.28.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gradio-5.15.0-py3-none-any.whl (57.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.8/57.8 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading gradio_client-1.7.0-py3-none-any.whl (321 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m321.9/321.9 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fastapi-0.115.8-py3-none-any.whl (94 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\nDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\nDownloading ruff-0.9.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\nDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nDownloading starlette-0.45.3-py3-none-any.whl (71 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\nDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\nInstalling collected packages: uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, starlette, safehttpx, gradio-client, fastapi, gradio, evaluate\n  Attempting uninstall: markupsafe\n    Found existing installation: MarkupSafe 3.0.2\n    Uninstalling MarkupSafe-3.0.2:\n      Successfully uninstalled MarkupSafe-3.0.2\nSuccessfully installed evaluate-0.4.3 fastapi-0.115.8 ffmpy-0.5.0 gradio-5.15.0 gradio-client-1.7.0 markupsafe-2.1.5 python-multipart-0.0.20 ruff-0.9.5 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.45.3 tomlkit-0.13.2 uvicorn-0.34.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Import necessary libraries\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering, Trainer, TrainingArguments, pipeline\nfrom datasets import load_dataset\nimport evaluate\nimport collections\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport gradio as gr\nimport wandb\nwandb.login(key=\"2693275e49d45f755cbbe2f5f277d11f9fb0a469\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T08:03:03.336436Z","iopub.execute_input":"2025-02-09T08:03:03.336807Z","iopub.status.idle":"2025-02-09T08:03:36.130699Z","shell.execute_reply.started":"2025-02-09T08:03:03.336779Z","shell.execute_reply":"2025-02-09T08:03:36.129995Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmohamed-hishamk90\u001b[0m (\u001b[33mmohamed-hishamk90-aiet\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# Load pre-trained model and tokenizer\nmodel_name = \"bert-base-cased\"\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T08:04:05.859784Z","iopub.execute_input":"2025-02-09T08:04:05.860562Z","iopub.status.idle":"2025-02-09T08:04:09.531533Z","shell.execute_reply.started":"2025-02-09T08:04:05.860526Z","shell.execute_reply":"2025-02-09T08:04:09.530845Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8131095ad66a4008961491b6c06c0d6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5dab482e92894fd6ad2d802f6d0202f0"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a19d5c2a14a445eab6d7bfc66bbdcae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"959627ca16e44159b1ab583c98619e3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aaa77b5ef9fe40eeb500be0f50a2d2e2"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Load SQUAD dataset \ndataset = load_dataset('squad')\ndataset['train'][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T08:04:16.685037Z","iopub.execute_input":"2025-02-09T08:04:16.685408Z","iopub.status.idle":"2025-02-09T08:04:24.285891Z","shell.execute_reply.started":"2025-02-09T08:04:16.685380Z","shell.execute_reply":"2025-02-09T08:04:24.285087Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.62k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85ea5ca01bf84219a39cb4bb544828c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/14.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22556fcd689d490f959224ad337ad34d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/1.82M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87b02772844e42a79e589b97e690f09f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90ed353b17be4b53b282efefb6eb7568"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0b04bde62d142c4aa65a896e6c2edf9"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"{'id': '5733be284776f41900661182',\n 'title': 'University_of_Notre_Dame',\n 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"max_length = 384\nstride = 128\n\ndef preprocess_training_examples(examples):\n    questions = [q.strip() for q in examples[\"question\"]]\n    inputs = tokenizer(\n        questions,\n        examples[\"context\"],\n        max_length=max_length,\n        truncation=\"only_second\",\n        stride=stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    offset_mapping = inputs.pop(\"offset_mapping\")\n    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n    answers = examples[\"answers\"]\n    start_positions = []\n    end_positions = []\n\n    for i, offset in enumerate(offset_mapping):\n        sample_idx = sample_map[i]\n        answer = answers[sample_idx]\n        start_char = answer[\"answer_start\"][0]\n        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n        sequence_ids = inputs.sequence_ids(i)\n\n        # Find the start and end of the context\n        idx = 0\n        while sequence_ids[idx] != 1:\n            idx += 1\n        context_start = idx\n        while sequence_ids[idx] == 1:\n            idx += 1\n        context_end = idx - 1\n\n        # If the answer is not fully inside the context, label is (0, 0)\n        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n            start_positions.append(0)\n            end_positions.append(0)\n        else:\n            # Otherwise it's the start and end token positions\n            idx = context_start\n            while idx <= context_end and offset[idx][0] <= start_char:\n                idx += 1\n            start_positions.append(idx - 1)\n\n            idx = context_end\n            while idx >= context_start and offset[idx][1] >= end_char:\n                idx -= 1\n            end_positions.append(idx + 1)\n\n    inputs[\"start_positions\"] = start_positions\n    inputs[\"end_positions\"] = end_positions\n    return inputs\n\n\ntrain_dataset = dataset[\"train\"].map(\n    preprocess_training_examples,\n    batched=True,\n    remove_columns=dataset[\"train\"].column_names,\n)\nlen(dataset[\"train\"]), len(train_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T08:04:41.469466Z","iopub.execute_input":"2025-02-09T08:04:41.469810Z","iopub.status.idle":"2025-02-09T08:05:23.904646Z","shell.execute_reply.started":"2025-02-09T08:04:41.469780Z","shell.execute_reply":"2025-02-09T08:05:23.903974Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1177f62ee7b4d7a8ba1a41f707d9077"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(87599, 88729)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"def preprocess_validation_examples(examples):\n    questions = [q.strip() for q in examples[\"question\"]]\n    inputs = tokenizer(\n        questions,\n        examples[\"context\"],\n        max_length=max_length,\n        truncation=\"only_second\",\n        stride=stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n    example_ids = []\n\n    for i in range(len(inputs[\"input_ids\"])):\n        sample_idx = sample_map[i]\n        example_ids.append(examples[\"id\"][sample_idx])\n\n        sequence_ids = inputs.sequence_ids(i)\n        offset = inputs[\"offset_mapping\"][i]\n        inputs[\"offset_mapping\"][i] = [\n            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n        ]\n\n    inputs[\"example_id\"] = example_ids\n    return inputs\n\nvalidation_dataset = dataset[\"validation\"].map(\n    preprocess_validation_examples,\n    batched=True,\n    remove_columns=dataset[\"validation\"].column_names,\n)\nlen(dataset[\"validation\"]), len(validation_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T08:05:31.420580Z","iopub.execute_input":"2025-02-09T08:05:31.420911Z","iopub.status.idle":"2025-02-09T08:05:37.738959Z","shell.execute_reply.started":"2025-02-09T08:05:31.420881Z","shell.execute_reply":"2025-02-09T08:05:37.738022Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10570 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d814feb79d946c8be477accca502a50"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(10570, 10822)"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"metric = evaluate.load(\"squad\")\n\nn_best = 20\nmax_answer_length = 30\n\ndef compute_metrics(start_logits, end_logits, features, examples):\n    example_to_features = collections.defaultdict(list)\n    for idx, feature in enumerate(features):\n        example_to_features[feature[\"example_id\"]].append(idx)\n\n    predicted_answers = []\n    for example in tqdm(examples):\n        example_id = example[\"id\"]\n        context = example[\"context\"]\n        answers = []\n\n        # Loop through all features associated with that example\n        for feature_index in example_to_features[example_id]:\n            start_logit = start_logits[feature_index]\n            end_logit = end_logits[feature_index]\n            offsets = features[feature_index][\"offset_mapping\"]\n\n            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # Skip answers that are not fully in the context\n                    if offsets[start_index] is None or offsets[end_index] is None:\n                        continue\n                    # Skip answers with a length that is either < 0 or > max_answer_length\n                    if (\n                        end_index < start_index\n                        or end_index - start_index + 1 > max_answer_length\n                    ):\n                        continue\n\n                    answer = {\n                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n                    }\n                    answers.append(answer)\n\n        # Select the answer with the best score\n        if len(answers) > 0:\n            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n            predicted_answers.append(\n                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n            )\n        else:\n            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n\n    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n    return metric.compute(predictions=predicted_answers, references=theoretical_answers)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T08:07:12.848794Z","iopub.execute_input":"2025-02-09T08:07:12.849187Z","iopub.status.idle":"2025-02-09T08:07:13.724027Z","shell.execute_reply.started":"2025-02-09T08:07:12.849157Z","shell.execute_reply":"2025-02-09T08:07:13.723188Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.53k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c5dde04c2c5475e9f86ac7dc5e6061f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/3.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a5fa615ab01405faed6f11449dc0ea6"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Fine-tune the model\nargs = TrainingArguments(\n    \"bert-finetuned-squad\",\n    evaluation_strategy=\"no\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    fp16=True,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=validation_dataset,\n    tokenizer=tokenizer,\n)\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T08:08:32.812295Z","iopub.execute_input":"2025-02-09T08:08:32.812694Z","iopub.status.idle":"2025-02-09T11:06:53.858235Z","shell.execute_reply.started":"2025-02-09T08:08:32.812666Z","shell.execute_reply":"2025-02-09T11:06:53.857272Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-10-ec9725fdffbc>:12: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250209_080833-3as4if4j</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mohamed-hishamk90-aiet/huggingface/runs/3as4if4j' target=\"_blank\">bert-finetuned-squad</a></strong> to <a href='https://wandb.ai/mohamed-hishamk90-aiet/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mohamed-hishamk90-aiet/huggingface' target=\"_blank\">https://wandb.ai/mohamed-hishamk90-aiet/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mohamed-hishamk90-aiet/huggingface/runs/3as4if4j' target=\"_blank\">https://wandb.ai/mohamed-hishamk90-aiet/huggingface/runs/3as4if4j</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='33276' max='33276' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [33276/33276 2:58:13, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>2.586900</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.630900</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.465400</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.381300</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>1.327800</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>1.294100</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>1.219000</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>1.193700</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>1.138400</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>1.166300</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>1.120500</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>1.149400</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>1.129700</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>1.064800</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>1.091100</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>1.110500</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>1.062400</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>1.087800</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>1.022600</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>1.058400</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>1.034200</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>1.025300</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>0.812300</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.724100</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>0.775200</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>0.731400</td>\n    </tr>\n    <tr>\n      <td>13500</td>\n      <td>0.782800</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>0.769200</td>\n    </tr>\n    <tr>\n      <td>14500</td>\n      <td>0.751600</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>0.754900</td>\n    </tr>\n    <tr>\n      <td>15500</td>\n      <td>0.752800</td>\n    </tr>\n    <tr>\n      <td>16000</td>\n      <td>0.784400</td>\n    </tr>\n    <tr>\n      <td>16500</td>\n      <td>0.752300</td>\n    </tr>\n    <tr>\n      <td>17000</td>\n      <td>0.760600</td>\n    </tr>\n    <tr>\n      <td>17500</td>\n      <td>0.748100</td>\n    </tr>\n    <tr>\n      <td>18000</td>\n      <td>0.731900</td>\n    </tr>\n    <tr>\n      <td>18500</td>\n      <td>0.729400</td>\n    </tr>\n    <tr>\n      <td>19000</td>\n      <td>0.702900</td>\n    </tr>\n    <tr>\n      <td>19500</td>\n      <td>0.738800</td>\n    </tr>\n    <tr>\n      <td>20000</td>\n      <td>0.776200</td>\n    </tr>\n    <tr>\n      <td>20500</td>\n      <td>0.782200</td>\n    </tr>\n    <tr>\n      <td>21000</td>\n      <td>0.726600</td>\n    </tr>\n    <tr>\n      <td>21500</td>\n      <td>0.739900</td>\n    </tr>\n    <tr>\n      <td>22000</td>\n      <td>0.702900</td>\n    </tr>\n    <tr>\n      <td>22500</td>\n      <td>0.581000</td>\n    </tr>\n    <tr>\n      <td>23000</td>\n      <td>0.518800</td>\n    </tr>\n    <tr>\n      <td>23500</td>\n      <td>0.543700</td>\n    </tr>\n    <tr>\n      <td>24000</td>\n      <td>0.533500</td>\n    </tr>\n    <tr>\n      <td>24500</td>\n      <td>0.487100</td>\n    </tr>\n    <tr>\n      <td>25000</td>\n      <td>0.527700</td>\n    </tr>\n    <tr>\n      <td>25500</td>\n      <td>0.522900</td>\n    </tr>\n    <tr>\n      <td>26000</td>\n      <td>0.482800</td>\n    </tr>\n    <tr>\n      <td>26500</td>\n      <td>0.517100</td>\n    </tr>\n    <tr>\n      <td>27000</td>\n      <td>0.496700</td>\n    </tr>\n    <tr>\n      <td>27500</td>\n      <td>0.522800</td>\n    </tr>\n    <tr>\n      <td>28000</td>\n      <td>0.493600</td>\n    </tr>\n    <tr>\n      <td>28500</td>\n      <td>0.492100</td>\n    </tr>\n    <tr>\n      <td>29000</td>\n      <td>0.511500</td>\n    </tr>\n    <tr>\n      <td>29500</td>\n      <td>0.523300</td>\n    </tr>\n    <tr>\n      <td>30000</td>\n      <td>0.522600</td>\n    </tr>\n    <tr>\n      <td>30500</td>\n      <td>0.506000</td>\n    </tr>\n    <tr>\n      <td>31000</td>\n      <td>0.485400</td>\n    </tr>\n    <tr>\n      <td>31500</td>\n      <td>0.505400</td>\n    </tr>\n    <tr>\n      <td>32000</td>\n      <td>0.502700</td>\n    </tr>\n    <tr>\n      <td>32500</td>\n      <td>0.503600</td>\n    </tr>\n    <tr>\n      <td>33000</td>\n      <td>0.471800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=33276, training_loss=0.8327280840831875, metrics={'train_runtime': 10700.5865, 'train_samples_per_second': 24.876, 'train_steps_per_second': 3.11, 'total_flos': 5.216534983896422e+16, 'train_loss': 0.8327280840831875, 'epoch': 3.0})"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Evaluation\npredictions, _, _ = trainer.predict(validation_dataset)\nstart_logits, end_logits = predictions\ncompute_metrics(start_logits, end_logits, validation_dataset, dataset[\"validation\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T11:07:11.599449Z","iopub.execute_input":"2025-02-09T11:07:11.599852Z","iopub.status.idle":"2025-02-09T11:10:00.951586Z","shell.execute_reply.started":"2025-02-09T11:07:11.599816Z","shell.execute_reply":"2025-02-09T11:10:00.950903Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10570 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ea7d58cf19d47ad9ed8d52c498dad9e"}},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"{'exact_match': 81.19205298013244, 'f1': 88.62792067792651}"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"def answer_question(question, context):\n    # Tokenize the question and context\n    inputs = tokenizer.encode_plus(\n        question,\n        context,\n        return_tensors=\"pt\",\n        max_length=384,\n        truncation=True,\n        padding=\"max_length\"\n    )\n\n    # Move inputs to the same device as the model\n    inputs = {key: val.to(model.device) for key, val in inputs.items()}\n\n    # Perform inference\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    # Extract start and end logits\n    start_logits = outputs.start_logits\n    end_logits = outputs.end_logits\n\n    # Find the tokens with the highest start and end scores\n    start_index = torch.argmax(start_logits)\n    end_index = torch.argmax(end_logits)\n\n    # Ensure the end index is not before the start index\n    if start_index > end_index:\n        return \"Invalid answer\"\n\n    # Decode the answer tokens\n    input_ids = inputs[\"input_ids\"][0].cpu().tolist()  # Move back to CPU for decoding\n    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n    answer_tokens = tokens[start_index:end_index + 1]\n    answer = tokenizer.convert_tokens_to_string(answer_tokens)\n\n    # Remove special tokens ([CLS], [SEP])\n    answer = answer.replace(\"[CLS]\", \"\").replace(\"[SEP]\", \"\").strip()\n\n    return answer\n\n# Example test\ncontext = \"The Eiffel Tower is a wrought-iron lattice tower located on the Champ de Mars in Paris, France. It was named after the engineer Gustave Eiffel, whose company designed and built the tower.\"\nquestion = \"Who designed and built the Eiffel Tower?\"\nanswer = answer_question(question, context)\nprint(f\"Question: {question}\")\nprint(f\"Answer: {answer}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T11:10:07.837477Z","iopub.execute_input":"2025-02-09T11:10:07.837773Z","iopub.status.idle":"2025-02-09T11:10:07.893486Z","shell.execute_reply.started":"2025-02-09T11:10:07.837749Z","shell.execute_reply":"2025-02-09T11:10:07.892845Z"}},"outputs":[{"name":"stdout","text":"Question: Who designed and built the Eiffel Tower?\nAnswer: Gustave Eiffel\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Save the fine-tuned model and tokenizer\nmodel.save_pretrained(\"/kaggle/working/bert-finetuned-squad\")\ntokenizer.save_pretrained(\"/kaggle/working/bert-finetuned-squad\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T11:21:20.939730Z","iopub.execute_input":"2025-02-09T11:21:20.940063Z","iopub.status.idle":"2025-02-09T11:21:21.963997Z","shell.execute_reply.started":"2025-02-09T11:21:20.940040Z","shell.execute_reply":"2025-02-09T11:21:21.963309Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/bert-finetuned-squad/tokenizer_config.json',\n '/kaggle/working/bert-finetuned-squad/special_tokens_map.json',\n '/kaggle/working/bert-finetuned-squad/vocab.txt',\n '/kaggle/working/bert-finetuned-squad/added_tokens.json',\n '/kaggle/working/bert-finetuned-squad/tokenizer.json')"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"!zip -r /kaggle/working/bert-finetuned-squad.zip /kaggle/working/bert-finetuned-squad","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T11:23:53.474624Z","iopub.execute_input":"2025-02-09T11:23:53.474948Z","iopub.status.idle":"2025-02-09T11:27:28.049271Z","shell.execute_reply.started":"2025-02-09T11:23:53.474924Z","shell.execute_reply":"2025-02-09T11:27:28.048496Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/bert-finetuned-squad/ (stored 0%)\n  adding: kaggle/working/bert-finetuned-squad/checkpoint-11092/ (stored 0%)\n  adding: kaggle/working/bert-finetuned-squad/checkpoint-11092/rng_state.pth (deflated 25%)\n  adding: kaggle/working/bert-finetuned-squad/checkpoint-11092/tokenizer.json (deflated 70%)\n  adding: kaggle/working/bert-finetuned-squad/checkpoint-11092/model.safetensors (deflated 7%)\n  adding: kaggle/working/bert-finetuned-squad/checkpoint-11092/config.json (deflated 48%)\n  adding: kaggle/working/bert-finetuned-squad/checkpoint-11092/training_args.bin (deflated 51%)\n  adding: kaggle/working/bert-finetuned-squad/checkpoint-11092/special_tokens_map.json (deflated 42%)\n  adding: kaggle/working/bert-finetuned-squad/checkpoint-11092/tokenizer_config.json (deflated 75%)\n  adding: kaggle/working/bert-finetuned-squad/checkpoint-11092/trainer_state.json (deflated 71%)\n  adding: kaggle/working/bert-finetuned-squad/checkpoint-11092/optimizer.pt (deflated 10%)\n  adding: kaggle/working/bert-finetuned-squad/checkpoint-11092/scheduler.pt (deflated 55%)\n  adding: kaggle/working/bert-finetuned-squad/checkpoint-11092/vocab.txt (deflated 49%)\n  adding: kaggle/working/bert-finetuned-squad/tokenizer.json (deflated 70%)\n  adding: kaggle/working/bert-finetuned-squad/model.safetensors (deflated 7%)\n  adding: kaggle/working/bert-finetuned-squad/checkpoint-33276/ (stored 0%)\n  adding: kaggle/working/bert-finetuned-squad/checkpoint-33276/rng_state.pth (deflated 25%)\n  adding: kaggle/working/bert-finetuned-squad/checkpoint-33276/tokenizer.json (deflated 70%)\n  adding: kaggle/working/bert-finetuned-squad/checkpoint-33276/model.safetensors (deflated 7%)\n  adding: kaggle/working/bert-finetuned-squad/checkpoint-33276/config.json (deflated 48%)\n  adding: kaggle/working/bert-finetuned-squad/checkpoint-33276/training_args.bin (deflated 51%)\n  adding: kaggle/working/bert-finetuned-squad/checkpoint-33276/special_tokens_map.json (deflated 42%)\n  adding: kaggle/working/bert-finetuned-squad/checkpoint-33276/tokenizer_config.json (deflated 75%)\n  adding: kaggle/working/bert-finetuned-squad/checkpoint-33276/trainer_state.json (deflated 75%)\n  adding: kaggle/working/bert-finetuned-squad/checkpoint-33276/optimizer.pt (deflated 10%)\n  adding: kaggle/working/bert-finetuned-squad/checkpoint-33276/scheduler.pt (deflated 56%)\n  adding: kaggle/working/bert-finetuned-squad/checkpoint-33276/vocab.txt (deflated 49%)\n  adding: kaggle/working/bert-finetuned-squad/runs/ (stored 0%)\n  adding: kaggle/working/bert-finetuned-squad/runs/Feb09_08-08-32_38edd5b31745/ (stored 0%)\n  adding: kaggle/working/bert-finetuned-squad/runs/Feb09_08-08-32_38edd5b31745/events.out.tfevents.1739088513.38edd5b31745.31.0 (deflated 66%)\n  adding: kaggle/working/bert-finetuned-squad/config.json (deflated 48%)\n  adding: kaggle/working/bert-finetuned-squad/checkpoint-22184/ (stored 0%)\n  adding: kaggle/working/bert-finetuned-squad/checkpoint-22184/rng_state.pth (deflated 25%)\n  adding: kaggle/working/bert-finetuned-squad/checkpoint-22184/tokenizer.json (deflated 70%)\n  adding: kaggle/working/bert-finetuned-squad/checkpoint-22184/model.safetensors (deflated 7%)\n  adding: kaggle/working/bert-finetuned-squad/checkpoint-22184/config.json (deflated 48%)\n  adding: kaggle/working/bert-finetuned-squad/checkpoint-22184/training_args.bin (deflated 51%)\n  adding: kaggle/working/bert-finetuned-squad/checkpoint-22184/special_tokens_map.json (deflated 42%)\n  adding: kaggle/working/bert-finetuned-squad/checkpoint-22184/tokenizer_config.json (deflated 75%)\n  adding: kaggle/working/bert-finetuned-squad/checkpoint-22184/trainer_state.json (deflated 73%)\n  adding: kaggle/working/bert-finetuned-squad/checkpoint-22184/optimizer.pt (deflated 10%)\n  adding: kaggle/working/bert-finetuned-squad/checkpoint-22184/scheduler.pt (deflated 55%)\n  adding: kaggle/working/bert-finetuned-squad/checkpoint-22184/vocab.txt (deflated 49%)\n  adding: kaggle/working/bert-finetuned-squad/special_tokens_map.json (deflated 42%)\n  adding: kaggle/working/bert-finetuned-squad/tokenizer_config.json (deflated 75%)\n  adding: kaggle/working/bert-finetuned-squad/vocab.txt (deflated 49%)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}